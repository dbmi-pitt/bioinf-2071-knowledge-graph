{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 1 Part III - KG searching, weighting, and embedding\n",
    "\n",
    "**Overview:** In this notebook, you will learn how to a couple of different ways search to through a large knowledge graph, how to reweight the knowledge graph to favor specific paths, and how to embed nodes in the knowledge graph to try and fill in gaps.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part I - a couple of different ways search to through a large knowledge graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These first cells simply set up some variables that will be used below\n",
    "\n",
    "**IMPORTANT:** You need to obtain the path to your user home folder on the CRC from the shell. You can simply run `pwd` and copy that to replace '/ihome/rboyce/rdb20/' in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "USER_HOME='/ihome/rboyce/rdb20/' ## REPLACE THIS WITH THE PATH TO YOUR HOME HOLDER IN THE CRC ENVIRONMENT \n",
    "sys.path.append(USER_HOME + '.conda/envs/bionf2071/lib/python3.7/site-packages/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There can be many paths of the same length so you can adjust how many you want returned\n",
    "maxNumPaths = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can populate the list in the next cell with the start and end nodes that you will use to search for paths in the knowledge graph. The [PheKnowLator overview](https://user-images.githubusercontent.com/8030363/103158881-11813b00-4780-11eb-8b45-5063765e7645.png) shows all of the ontologies loaded into the knowledge graph as orange boxes. You can replace the start and end nodes with entities that interest you in those ontologies by going to [Ontobee](http://www.ontobee.org/) and obtaining the URIs and then replacing the start and end node pairs in the s_o_tpl_L list. Be sure to set FIRST_TO_PROCESS to the first pair. If you have to restart the knowledge graph searches shown later, you can change that variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_o_tpl_L = [\n",
    "    (\"http://purl.obolibrary.org/obo/HP_0000716\",\"http://purl.obolibrary.org/obo/DOID_680\"),\n",
    "    (\"http://purl.obolibrary.org/obo/HP_0000716\",\"http://purl.obolibrary.org/obo/HP_0001297\"),\n",
    "    (\"http://purl.obolibrary.org/obo/HP_0000716\",\"http://purl.obolibrary.org/obo/HP_0001635\"),\n",
    "    (\"http://purl.obolibrary.org/obo/HP_0000716\",\"http://purl.obolibrary.org/obo/HP_0002383\"),\n",
    "    (\"http://purl.obolibrary.org/obo/HP_0000716\",\"http://purl.obolibrary.org/obo/HP_0002621\")\n",
    "]\n",
    "## Leave as None unless you have to restart this cell before it has completely processed all tuples.\n",
    "## Otherwise, replace this tuple with the first tuple to process completely by the program \n",
    "FIRST_TO_PROCESS =  (\"http://purl.obolibrary.org/obo/HP_0000716\",\"http://purl.obolibrary.org/obo/DOID_680\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The libraries needed to import the graph and work with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path\n",
    "import networkx as nx\n",
    "import json\n",
    "import urllib\n",
    "import traceback\n",
    "import sys\n",
    "from itertools import islice\n",
    "from rdflib import Graph, URIRef, BNode, Namespace, Literal\n",
    "from rdflib.namespace import RDF, OWL\n",
    "import smart_open\n",
    "from node2vec import Node2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two cell are the graph loading step. The graph is the output of the PheKnowLator process including filtering out OWL semantics. The sources used in this version of the knowledge graph are listed on [this Wiki page](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources). The graph takes more than 15 gigs of RAM once loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPHPATH = \"PheKnowLator_full_InverseRelations_NotClosed_OWLNETS_Networkx_MultiDiGraph_REWEIGHT_NO_DISJOINT.gpickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading the graph produced from the PheKnowLator workflow\n",
    "# The graph \n",
    "nx_mdg = nx.read_gpickle(GRAPHPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell defines several functions that will help to explain the results of path searches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ontobee_service = \"http://sparql.hegroup.org/sparql/\"\n",
    "\n",
    "\n",
    "def query(q,epr,f='application/sparql-results+json'):\n",
    "    \"\"\"Function that uses urllib/urllib2 to issue a SPARQL query.\n",
    "       By default it requests json as data format for the SPARQL resultset\"\"\"\n",
    "\n",
    "    try:\n",
    "        params = {'query': q}\n",
    "        params = urllib.parse.urlencode(params)\n",
    "        opener = urllib.request.build_opener(urllib.request.HTTPHandler)\n",
    "        request = urllib.request.Request(epr+'?'+params)\n",
    "        request.add_header('Accept', f)\n",
    "        request.get_method = lambda: 'GET'\n",
    "        url = opener.open(request)\n",
    "        return url.read()\n",
    "    except Exception as e:\n",
    "        traceback.print_exc(file=sys.stdout)\n",
    "        raise e\n",
    "\n",
    "\n",
    "def pathQuery(pth):\n",
    "    \"\"\"Given a single path (list of rdflib objects), run a sparql query to retrieve descriptive information \n",
    "       that will help construct a narrative explanation\"\"\"\n",
    "    \n",
    "    uriL = [x.toPython() for x in filter(lambda x: type(x) == URIRef, pth)]\n",
    "    subjectStr = ''\n",
    "    objectStr = ''\n",
    "    for uri in uriL:\n",
    "        subjectStr = subjectStr + '?s = <' + uri + '> || '\n",
    "        objectStr = objectStr + '?o = <' + uri + '> || '\n",
    "    subjectStr = subjectStr[:-3] # ?s = <..> || ?s = <...>... the URIs for the subject/object entities in the path\n",
    "    objectStr = objectStr[:-3] # ?o = <..> || ?o = <...>... the URIs for the subject/object entities in the path\n",
    "   \n",
    "    q = '''\n",
    "  prefix obo:<http://purl.obolibrary.org/obo/>\n",
    "  prefix owl:<http://www.w3.org/2002/07/owl#>\n",
    " \n",
    "  select distinct ?s ?p ?o ?p_lab ?s_lab_eg ?o_lab_eg\n",
    "  from <http://phenowlator_merged.owl>\n",
    "  from <ro_with_imports_AD_mods>\n",
    "  where {{\n",
    "    ?s ?p ?o.FILTER(({}) && ({})) \n",
    "    OPTIONAL{{\n",
    "     ?p rdfs:label ?p_lab.\n",
    "   }}\n",
    "   OPTIONAL{{\n",
    "     ?egM_s <http://dikb.org/ad#obo_mapping> ?s.\n",
    "     ?egM_s rdfs:label ?s_lab_eg.\n",
    "   }}\n",
    "   OPTIONAL{{\n",
    "     ?egM_o <http://dikb.org/ad#obo_mapping> ?o.\n",
    "     ?egM_o rdfs:label ?o_lab_eg.\n",
    "   }}\n",
    "  }}\n",
    "'''.format(subjectStr,objectStr)\n",
    "    \n",
    "    return q\n",
    "\n",
    "\n",
    "def missingLabelQuery(uri, endpoint):\n",
    "    query_string = '''\n",
    "SELECT distinct ?lab\n",
    "WHERE {{ \n",
    " <{}> rdfs:label ?lab. \n",
    "}}\n",
    "'''.format(uri)\n",
    "    json_string = query(query_string, endpoint)\n",
    "    resultset = json.loads(json_string)\n",
    "    rsltsD = {}\n",
    "    for b in resultset[\"results\"][\"bindings\"]:\n",
    "        if not b.get('lab'):\n",
    "            return None\n",
    "        \n",
    "        label = b['lab']['value']\n",
    "        if label:\n",
    "            return label\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "def constructPathNarData(pth, sparql_service, debug=False):\n",
    "    \"\"\" Iterate through the path to organize a narrative\n",
    "        in: pth - a list of rdflib URIRef and BNode objects\n",
    "        in: sparql_service - URL to the sparql endpoint\n",
    "    \"\"\"\n",
    "    query_string = pathQuery(pth)\n",
    "    if debug:\n",
    "        print('[DEBUG] Query:\\n' + query_string)\n",
    "        \n",
    "    json_string = query(query_string, pheknowlator_service)\n",
    "    resultset = json.loads(json_string)\n",
    "    print(\"[INFO] Number of results: \" + str(len(resultset[\"results\"][\"bindings\"])))\n",
    "    \n",
    "    # Re-organize the results set to be a dict keyed by the subject uri\n",
    "    rsltsD = {}\n",
    "    for b in resultset[\"results\"][\"bindings\"]:\n",
    "        s = b['s']['value']\n",
    "        if rsltsD.get(s):\n",
    "            rsltsD[s].append(b)\n",
    "        else:\n",
    "            rsltsD[s] = [b]\n",
    "    \n",
    "    narrative = \"\"     \n",
    "    for i in range(0,len(pth)):\n",
    "        if i == len(pth) - 1:\n",
    "            break\n",
    "        \n",
    "        n1 = pth[i]\n",
    "        n2 = pth[i+1]\n",
    "    \n",
    "        if not (type(n1) == URIRef and type(n2) == URIRef):\n",
    "            narrative += \"----- BLANK NODE STEP ----\"\n",
    "            continue\n",
    "        else:\n",
    "            # locate the results dict that relates n1 to n2 in a subject, predicate, object triple\n",
    "            o_d = None\n",
    "            for d in rsltsD.get(n1.toPython()):\n",
    "                if d['o']['value'] == n2.toPython():\n",
    "                    o_d = d\n",
    "                    break\n",
    "        \n",
    "            if not o_d:\n",
    "                print(\"ERROR: Unable to find a triple relating {} to {} in path {}\".format(n1.toPython(),n2.toPython(),pth))\n",
    "            else:\n",
    "                o_lab = '<no label found>'\n",
    "                o_node_id = o_d['o']['value'].split('/')[-1]\n",
    "                if nodeLabD.get(o_node_id):\n",
    "                    o_lab = nodeLabD[o_node_id]\n",
    "                else:\n",
    "                    if label_cache.get(o_d['o']['value']):\n",
    "                        o_lab = label_cache[o_d['o']['value']]\n",
    "                    else:\n",
    "                        ql = missingLabelQuery(o_d['o']['value'],ontobee_service)\n",
    "                        if ql:\n",
    "                            label_cache[o_d['o']['value']] = ql\n",
    "                            o_lab = ql\n",
    "                    \n",
    "                    if o_lab == '<no label found>' and o_d.get('o_lab_eg'):\n",
    "                        o_lab = o_d['o_lab_eg']['value'] + ' (UMLS label)'\n",
    "\n",
    "                s_lab = '<no label found>'\n",
    "                s_node_id = o_d['s']['value'].split('/')[-1]\n",
    "                if nodeLabD.get(s_node_id):\n",
    "                    s_lab = nodeLabD[s_node_id]\n",
    "                else:\n",
    "                    if label_cache.get(o_d['s']['value']):\n",
    "                        s_lab = label_cache[o_d['s']['value']]\n",
    "                    else:\n",
    "                        ql = missingLabelQuery(o_d['s']['value'],ontobee_service)\n",
    "                        if ql:\n",
    "                            label_cache[o_d['s']['value']] = ql\n",
    "                            s_lab = ql  \n",
    "                    \n",
    "                    if s_lab == '<no label found>' and o_d.get('s_lab_eg'):\n",
    "                        s_lab = o_d['s_lab_eg']['value'] + ' (UMLS label)'\n",
    "\n",
    "                if o_d.get('p_lab'):                      \n",
    "                    narrative += '{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n'.format(\n",
    "                            s_lab,o_d['p_lab']['value'],o_lab,\n",
    "                            o_d['s']['value'],o_d['p']['value'],o_d['o']['value']\n",
    "                           )                          \n",
    "                else:\n",
    "                    narrative += '{}\\t{}\\t{}\\t{}\\t{}\\t{}\\n'.format(\n",
    "                            s_lab,o_d['p']['value'].replace('http://www.w3.org/2000/01/rdf-schema#',''),o_lab,\n",
    "                            o_d['s']['value'],o_d['p']['value'],o_d['o']['value']\n",
    "                           )                                          \n",
    "    return narrative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell loads a file that has all of the node labels which were stripped from the graph when OWL semantics were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeLabD = {}\n",
    "f = open('PheKnowLator_full_InverseRelations_NotClosed_NoOWLSemantics_NodeLabels.txt','r')\n",
    "buf = f.read()\n",
    "f.close()\n",
    "nodLabL = buf.split('\\n')\n",
    "for line in nodLabL:\n",
    "    spL = line.split('\\t')\n",
    "    if len(spL) > 1:\n",
    "        nodeLabD[spL[0]] = spL[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function conducts a shortest path search given a graph source and target k results are returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_shortest_paths(G, source, target, k, weight='weight'):\n",
    "    return list(islice(nx.all_shortest_paths(G, source, target, weight=weight), k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function conducts the simple path searches given a graph source and target and an integers with the shortest path length known \n",
    "The shortest path lengths is used so that already seen simple paths are not returned\n",
    "k results are returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_simple_paths(G, source, target, k, shortestLen):\n",
    "    paths = nx.all_simple_paths(G, source, target, cutoff=shortestLen+20)\n",
    "    path_l = []\n",
    "    i = 0\n",
    "    while i < k:\n",
    "        try:\n",
    "            print('[info] applying next operator to search for a simple path of max length {}'.format(shortestLen+20))\n",
    "            path = next(paths)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        print('[info] Simple path found of length {}'.format(len(path))) \n",
    "        if len(path) > shortestLen:\n",
    "            print('[info] Simple path length greater than shortest path length ({}) so adding to results'.format(shortestLen))\n",
    "            path_l.append(path)\n",
    "        i += 1\n",
    "    return path_l      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the endpoint where an RDF version of the PheKnowLator graph resides\n",
    "pheknowlator_service = \"https://dbmi-icode-01.dbmi.pitt.edu/sparql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There can be allot of repetition when doing certain search patterns (e.g., confounders) so set up a cache \n",
    "processed_tpl_cache = []\n",
    "# queried label cache\n",
    "label_cache = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell conducts the shortest path searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortestPathsLens = [] # we track the shortest paths from source to target for when we do simple path searches\n",
    "for tpl in s_o_tpl_L:      \n",
    "    if FIRST_TO_PROCESS:\n",
    "        if str(tpl) != str(FIRST_TO_PROCESS):\n",
    "            print('INFO: skipping tuple because it is not FIRST_TO_PROCESS:' + str(tpl))\n",
    "            continue\n",
    "        else:\n",
    "            FIRST_TO_PROCESS = None\n",
    "    \n",
    "    (s,o) = tpl    \n",
    "    startNd = URIRef(s) \n",
    "    endNd = URIRef(o) \n",
    "       \n",
    "    print('INFO: Processing {} and {}:\\n'.format(s,o))\n",
    "                \n",
    "    try:\n",
    "        pthL = k_shortest_paths(nx_mdg,startNd,endNd,maxNumPaths)\n",
    "    except nx.NetworkXNoPath:\n",
    "        print('INFO: No results in the path search.\\n')\n",
    "        continue\n",
    "    except nx.NodeNotFound:\n",
    "        print('INFO: The source node does not exist in the Knowledge Graph.\\n')\n",
    "        continue\n",
    "\n",
    "    narL = []\n",
    "    c = -1\n",
    "    for path in pthL:\n",
    "        if c == -1:\n",
    "            shortestPathsLens.append(len(path))\n",
    "            \n",
    "        c += 1\n",
    "        nar = constructPathNarData(path,pheknowlator_service,debug=False)\n",
    "        print('\\n\\nINFO: SHORTEST PATH {}:\\n'.format(c))\n",
    "        print(nar)\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell conducts the simple path searches but in a way to ignores the shortest paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxNumPaths = 3  # we search for fewer paths b/c this can take much longer than the shortest path searches  \n",
    "tplCnt = 0\n",
    "for tpl in s_o_tpl_L:      \n",
    "    if FIRST_TO_PROCESS:\n",
    "        if str(tpl) != str(FIRST_TO_PROCESS):\n",
    "            print('INFO: skipping tuple because it is not FIRST_TO_PROCESS:' + str(tpl))\n",
    "            continue\n",
    "        else:\n",
    "            FIRST_TO_PROCESS = None\n",
    "    \n",
    "    (s,o) = tpl    \n",
    "    startNd = URIRef(s) \n",
    "    endNd = URIRef(o) \n",
    "       \n",
    "    print('INFO: Processing {} and {}:\\n'.format(s,o))\n",
    "                \n",
    "    try:\n",
    "        pthL = k_simple_paths(nx_mdg, startNd, endNd, maxNumPaths, shortestPathsLens[tplCnt])\n",
    "        tplCnt += 1\n",
    "    except nx.NetworkXNoPath:\n",
    "        print('INFO: No results in the path search.\\n')\n",
    "        continue\n",
    "    except nx.NodeNotFound:\n",
    "        print('INFO: The source node does not exist in the Knowledge Graph.\\n')\n",
    "        continue\n",
    "\n",
    "    narL = []\n",
    "    c = -1\n",
    "    for path in pthL:\n",
    "        c += 1\n",
    "        nar = constructPathNarData(path,pheknowlator_service,debug=False)\n",
    "        print('\\n\\nINFO: SIMPLE PATH {}:\\n'.format(c))\n",
    "        print(nar)\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E7** - Questions about path searches\n",
    "\n",
    "1. In part II of Assignment I you saw some SPARQL queries over a small knowledge graph created from predications extracted by machine reading with SemMedDB. In this part of the Assignment I you are working with a much larger and more complex graph constructed using ontologies and several large data sources (go back and review sources used in this version of the knowledge graph are listed on [this Wiki page](https://github.com/callahantiff/PheKnowLator/wiki/v2-Data-Sources)). We could SPARQL for this graph but we are using graph search instead. Make some obervations in your own words about some differences between the two approaches. \n",
    "\n",
    "2. Take a close look at the results of the shortest path searches and make some comments about how potentially factual and useful the path narratives would be for providing a mechanistic explanation of the relationship between the start and end nodes. \n",
    "\n",
    "3. Similar to the previous question, take a close look at the results of the SIMPLE path searches and make some comments about how potentially factual and useful the path narratives would be for providing a mechanistic explanation of the relationship between the start and end nodes. Also, what differences between the simple and shortes path searches stand out to you.\n",
    "\n",
    "4. Think back to the previous part of Assignment I where we saw transitive and symmetric closure over a small knowledge graph created from SemMedDB predications. The PheKnoLator knowledge graph we are working with here is built as a formal ontology represented in OWL. Can you suggest how to approach closure for this large knowledge graph (hint: think about what can be done with description logic)? Also, explain how you think closure would be important for the path search approach you see in this notebook. \n",
    " \n",
    "6. Do you have any questions for me about this section?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part II - how to reweight the knowledge graph to favor specific paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of graph metrics that could be use to reweight edges in the graph (see https://networkx.org/documentation/stable/reference/algorithms/centrality.html) Here, we pick a simple measure to calculate called degree centrality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## obtain a dictionary of node degree centrality for all nodes using the default parameters\n",
    "centrality = nx.degree_centrality(nx_mdg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Alzheimer's disease as an example \n",
    "centrality[URIRef('http://purl.obolibrary.org/obo/HP_0002511')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Cell as an example\n",
    "centrality[URIRef('http://purl.obolibrary.org/obo/CL_0000000')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use node degree centrality when reweighting the graph so we need to calculate the measure for all nodes\n",
    "f = open('centrality.tsv','w')\n",
    "f.write('node\\tdegree_centrality\\n')\n",
    "for k,v in centrality.items():\n",
    "    f.write('{}\\t{}\\n'.format(k.toPython(),v))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell creates a new graph  copying and reweighting the current graph using degree centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_mdg_cntr = nx.MultiDiGraph()\n",
    "\n",
    "# the reweight graph will remove 'disjoint' and 'domain' predicates  and apply a \n",
    "edge_key = -1 # an int that we will increment to uniquely identify edges\n",
    "for s, o, data in nx_mdg.edges(data=True):\n",
    "    p = data['predicate']\n",
    "    \n",
    "    edge_key += 1\n",
    "    \n",
    "    ## default weight = 2\n",
    "    weight = 2.0\n",
    "    \n",
    "    # Skip disjoint and domain predicates\n",
    "    if p.toPython() == 'http://www.w3.org/2002/07/owl#disjointWith' or p.toPython() == 'http://www.w3.org/2000/01/rdf-schema#domain':\n",
    "        continue    \n",
    "           \n",
    "    # Add to the weight the degree centrality of each node\n",
    "    if centrality.get(p):\n",
    "        weight = weight + centrality[p]\n",
    "    if centrality.get(o):\n",
    "        weight = weight + centrality[o]\n",
    "    \n",
    "    # add the edge to the graph giving it a unique key and weight\n",
    "    nx_mdg_cntr.add_edge(s, o, **{'predicate': p,'key': edge_key, 'weight':weight})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gpickle(nx_mdg_cntr,'PheKnowLator_full_InverseRelations_NotClosed_OWLNETS_DEGREE_CNTRLTY_REWEIGHT_NO_DISJOINT.gpickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct shortest and simple path searches with the reweighted graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxNumPaths = 3 \n",
    "shortestPathsLens = [] # we track the shortest paths from source to target for when we do simple path searches\n",
    "for tpl in s_o_tpl_L:      \n",
    "    if FIRST_TO_PROCESS:\n",
    "        if str(tpl) != str(FIRST_TO_PROCESS):\n",
    "            print('INFO: skipping tuple because it is not FIRST_TO_PROCESS:' + str(tpl))\n",
    "            continue\n",
    "        else:\n",
    "            FIRST_TO_PROCESS = None\n",
    "    \n",
    "    (s,o) = tpl    \n",
    "    startNd = URIRef(s) \n",
    "    endNd = URIRef(o) \n",
    "       \n",
    "    print('INFO: Processing {} and {}:\\n'.format(s,o))\n",
    "                \n",
    "    try:\n",
    "        pthL = k_shortest_paths(nx_mdg_cntr,startNd,endNd,maxNumPaths)\n",
    "    except nx.NetworkXNoPath:\n",
    "        print('INFO: No results in the path search.\\n')\n",
    "        continue\n",
    "    except nx.NodeNotFound:\n",
    "        print('INFO: The source node does not exist in the Knowledge Graph.\\n')\n",
    "        continue\n",
    "\n",
    "    narL = []\n",
    "    c = -1\n",
    "    for path in pthL:\n",
    "        if c == -1:\n",
    "            shortestPathsLens.append(len(path))\n",
    "            \n",
    "        c += 1\n",
    "        nar = constructPathNarData(path,pheknowlator_service,debug=False)\n",
    "        print('\\n\\nINFO: SHORTEST PATH {}:\\n'.format(c))\n",
    "        print(nar)\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct SIMPLE path searches with the reweighted graph\n",
    "maxNumPaths = 3  \n",
    "tplCnt = 0\n",
    "for tpl in s_o_tpl_L:      \n",
    "    if FIRST_TO_PROCESS:\n",
    "        if str(tpl) != str(FIRST_TO_PROCESS):\n",
    "            print('INFO: skipping tuple because it is not FIRST_TO_PROCESS:' + str(tpl))\n",
    "            continue\n",
    "        else:\n",
    "            FIRST_TO_PROCESS = None\n",
    "    \n",
    "    (s,o) = tpl    \n",
    "    startNd = URIRef(s) \n",
    "    endNd = URIRef(o) \n",
    "       \n",
    "    print('INFO: Processing {} and {}:\\n'.format(s,o))\n",
    "                \n",
    "    try:\n",
    "        pthL = k_simple_paths(nx_mdg_cntr, startNd, endNd, maxNumPaths, shortestPathsLens[tplCnt])\n",
    "        tplCnt += 1\n",
    "    except nx.NetworkXNoPath:\n",
    "        print('INFO: No results in the path search.\\n')\n",
    "        continue\n",
    "    except nx.NodeNotFound:\n",
    "        print('INFO: The source node does not exist in the Knowledge Graph.\\n')\n",
    "        continue\n",
    "\n",
    "    narL = []\n",
    "    c = -1\n",
    "    for path in pthL:\n",
    "        c += 1\n",
    "        nar = constructPathNarData(path,pheknowlator_service,debug=False)\n",
    "        print('\\n\\nINFO: SIMPLE PATH {}:\\n'.format(c))\n",
    "        print(nar)\n",
    "        print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E8** - questions about graph editing and reweighting\n",
    "\n",
    "1. Think about how degree centrality is being used here to reweight edges in the graph. If the goal is to obtain mechanistically accurate narratives, does using this measure to reweight make sense?\n",
    "\n",
    "2. Imagine that the goal is to avoid the inclusion several 'hub' nodes in path results. Look over the [centrality measures](https://networkx.org/documentation/stable/reference/algorithms/centrality.html) and justify if another which measure(s) might be better than degree centraity to accomplish this task.\n",
    "\n",
    "3. How could you use knowledge about the Relation Ontology to bias path search results to favor certain kinds of nodes (e.g., pathways) over others (e.g., genes) (hint: look at the ontologies and RO entities [here](https://user-images.githubusercontent.com/8030363/103158881-11813b00-4780-11eb-8b45-5063765e7645.png))?\n",
    "\n",
    "4. Pick three different centrality measures and explain the computational complexity tradeoffs of each.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part III - how to embed nodes in the knowledge graph to try and fill in gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to tackle link prediction as a supervised learning problem on top of node representations/embeddings. The embeddings are computed with the unsupervised node2vec algorithm. After obtaining embeddings, a binary classifier can be used to predict a link, or not, between any two nodes in the graph. Various hyperparameters could be relevant in obtaining the best link classifier -\n",
    "\n",
    "There are four steps:\n",
    "\n",
    "1. Obtain embeddings for each node\n",
    "2. For each set of hyperparameters, train a classifier\n",
    "3. Select the classifier that performs the best\n",
    "4. Evaluate the selected classifier on unseen data to validate its ability to generalise\n",
    "\n",
    "This part of the notebook has been adapted from a demo of the StellarGraph library (https://stellargraph.readthedocs.io/en/stable/index.html) for link prediction using the node2vec algorithm.\n",
    "\n",
    "<a name=\"refs\"></a>\n",
    "**References:** \n",
    "\n",
    "[1] Node2Vec: Scalable Feature Learning for Networks. A. Grover, J. Leskovec. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we derive a simple undirected graph that has only predicates (edges) indicitive of interaction:\n",
    "\n",
    "http://purl.obolibrary.org/obo/RO_0002434 - interacts with\n",
    "\n",
    "http://purl.obolibrary.org/obo/RO_0002436 - molecularly interacts with\n",
    "\n",
    "http://purl.obolibrary.org/obo/RO_0002435 - geneticly interacts with\n",
    "\n",
    "http://purl.obolibrary.org/obo/RO_0002437 - biotically interacts with\n",
    "\n",
    "http://purl.obolibrary.org/obo/RO_0000056 - partipates in\n",
    "\n",
    "Note: We limit the graph size to be a few tens of thousands of nodes (the full graph is >200K nodes) so that embedding can happen in a relatively short amount of time \n",
    "\n",
    "We also need to be choosy about the nodes we select because there are some more general nodes that have many thousands of interaction relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx_mdg_interacts = nx.Graph()\n",
    "\n",
    "object_nodeD = {} # tracking object nodes so we can add only those that are the object of 2 or more interactions\n",
    "maxNodes = 6000 # the max count of unique subject nodes\n",
    "edge_key = -1 # an int that we will increment to uniquely identify edges\n",
    "s_node_visitedD = {} # tracks unique subject nodes \n",
    "s_count = 0 # some entitites have a huge number of interactions so we will only sample a max of 100 for any entity\n",
    "s_current = None\n",
    "for s, o, data in nx_mdg.edges(data=True):\n",
    "    if not s_node_visitedD.get(s):\n",
    "        s_node_visitedD[s] = 1\n",
    "        if len(s_node_visitedD) == maxNodes:\n",
    "            break\n",
    "    \n",
    "    if s == s_current:\n",
    "        s_count += 1\n",
    "        if s_count > 50:\n",
    "            continue\n",
    "    else:\n",
    "        s_current = s\n",
    "        s_count = 0 \n",
    "        \n",
    "    p = data['predicate']\n",
    "    \n",
    "    edge_key += 1\n",
    "    \n",
    "    if not (p.toPython() == 'http://purl.obolibrary.org/obo/RO_0002434' # interacts with\n",
    "            or p.toPython() == 'http://purl.obolibrary.org/obo/RO_0002436' # molecularly interacts with\n",
    "            or p.toPython() == 'http://purl.obolibrary.org/obo/RO_0002435' # genetically interacts with\n",
    "            or p.toPython() == 'http://purl.obolibrary.org/obo/RO_0002437' # biotically interacts with \n",
    "            or p.toPython() == 'http://purl.obolibrary.org/obo/RO_0000056' # partipates in\n",
    "           ):\n",
    "        continue    \n",
    "    \n",
    "    if object_nodeD.get(o):\n",
    "        object_nodeD[o] += 1\n",
    "        \n",
    "        # add the edge to the graph giving it a unique key \n",
    "        nx_mdg_interacts.add_edge(s, o, **{'predicate': p,'key': edge_key})\n",
    "        \n",
    "    else:\n",
    "        object_nodeD[o] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gpickle(nx_mdg_interacts,'PheKnowLator_full_InverseRelations_NotClosed_OWLNETS_INTERACTS.gpickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment this to reload the 'interacts only' graph from file if needed\n",
    "# nx_mdg_interacts = nx.read_gpickle('PheKnowLator_full_InverseRelations_NotClosed_OWLNETS_INTERACTS.gpickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few cells give some insight into the content of our interaction sub-graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.info(nx_mdg_interacts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0 \n",
    "for d in nx_mdg_interacts.degree():\n",
    "    if d[1] > 2:        \n",
    "        print(d)\n",
    "        i += 1\n",
    "        if i == 100:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sample of what this new graph looks like\n",
    "i = 0\n",
    "for s, o, data in nx_mdg_interacts.edges(data=True):\n",
    "    if i == 200:\n",
    "        break\n",
    "    \n",
    "    print('{}\\t{}\\t{}'.format(s,o,data))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node2Vec\n",
    "\n",
    "We use Node2Vec [[1]](#refs), to calculate node embeddings. These embeddings are learned in such a way to ensure that nodes that are close in the graph remain close in the embedding space. Node2Vec first involves running random walks on the graph to obtain our context pairs, and using these to train a Word2Vec model.\n",
    "\n",
    "These are the set of parameters we can use:\n",
    "\n",
    "* `p` - Random walk parameter \"p\"\n",
    "* `q` - Random walk parameter \"q\"\n",
    "* `dimensions` - Dimensionality of node2vec embeddings\n",
    "* `num_walks` - Number of walks from each node\n",
    "* `walk_length` - Length of each random walk\n",
    "* `window_size` - Context window size for Word2Vec\n",
    "* `num_iter` - number of SGD iterations (epochs)\n",
    "* `workers` - Number of workers for Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few steps involved in using the model to perform link prediction:\n",
    "1. We calculate link/edge embeddings for the positive and negative edge samples by applying a binary operator on the embeddings of the source and target nodes of each sampled edge.\n",
    "2. Given the embeddings of the positive and negative examples, we train a logistic regression classifier to predict the probability indicating whether an edge between two nodes should exist or not.\n",
    "3. To use the embeddings in a traditional machine learning classifier, we use binary operators on the embeddings such as average, Hadamard, L1, L2. Here we use the 'average operator' with node embeddings in the logistic regression classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cells aply the node2vec to conctruc a 30 dimension embedding of the interaction sub-graph using 5 random walks per node with up to 10 node steps each. These hyperparameters are not tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2vec = Node2Vec(nx_mdg_interacts, dimensions=30, walk_length=10, num_walks=5, workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('PheKnowLator_full_InverseRelations_NotClosed_OWLNETS_INTERACTS.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment this and run it to reload the model from a file if you have already created it\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load('PheKnowLator_full_InverseRelations_NotClosed_OWLNETS_INTERACTS.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can examine the embedding vectors for any node in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entinostat\n",
    "model.wv.get_vector('http://purl.obolibrary.org/obo/CHEBI_132082')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('http://purl.obolibrary.org/obo/CHEBI_132082')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive regulation of nervous system development\n",
    "model.wv.get_vector('http://purl.obolibrary.org/obo/GO_0051962')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also seek the most similar nodes to a given ndode in the embedding space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('http://purl.obolibrary.org/obo/GO_0051962')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive=purine ribonucleoside binding and negative=positive regulation of nervous system development\n",
    "model.wv.most_similar(positive=['http://purl.obolibrary.org/obo/GO_0032550'],negative=['http://purl.obolibrary.org/obo/GO_0051962'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cells train a model for link prediction using the embeddings. We start by building an adjacency matrix and then traversing it to find the positions of the zeros which represent node pairs that are not connected in the graph.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps shown here follow the same procedure used in this [nice tutorial by Prateek Joshi](https://www.analyticsvidhya.com/blog/2020/01/link-prediction-how-to-predict-your-future-connections-on-facebook/) but applied to the interaction sub-graph of the PheKnowLator graph.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodelist = []\n",
    "nds = nx_mdg_interacts.nodes()\n",
    "initial_node_count = nx.number_of_nodes(nx_mdg_interacts)\n",
    "for n in nds:\n",
    "    nodelist.append(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build adjacency matrix\n",
    "adj_G = nx.to_numpy_matrix(nx_mdg_interacts,nodelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_G.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get unconnected node-pairs\n",
    "all_unconnected_pairs = [None]*(adj_G.shape[0]*adj_G.shape[1])\n",
    "all_connected_pairs = [None]*(adj_G.shape[0]*adj_G.shape[1])\n",
    "\n",
    "# traverse adjacency matrix (iterate columns by rows)\n",
    "offset = 0\n",
    "for i in range(adj_G.shape[0]):\n",
    "  for j in range(offset,adj_G.shape[1]):\n",
    "    if i != j:\n",
    "        if adj_G[i,j] == 0:\n",
    "          all_unconnected_pairs[i+j] = (nodelist[i],nodelist[j])\n",
    "        else:            \n",
    "          all_connected_pairs[i+j] = (nodelist[i],nodelist[j])\n",
    "\n",
    "  offset = offset + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "all_connected_pairs_clean = [x for x in filter(lambda x: x != None, all_connected_pairs)]\n",
    "node_1_linked = [i[0] for i in all_connected_pairs_clean]\n",
    "node_2_linked = [i[1] for i in all_connected_pairs_clean]\n",
    "original_g_df = pd.DataFrame({'node_1':node_1_linked, \n",
    "                              'node_2':node_2_linked})\n",
    "\n",
    "all_unconnected_pairs_clean = [x for x in filter(lambda x: x != None, all_unconnected_pairs)]\n",
    "node_1_unlinked = [i[0] for i in all_unconnected_pairs_clean]\n",
    "node_2_unlinked = [i[1] for i in all_unconnected_pairs_clean]\n",
    "data = pd.DataFrame({'node_1':node_1_unlinked, \n",
    "                     'node_2':node_2_unlinked})\n",
    "\n",
    "# add target variable 'link'\n",
    "data['link'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('original_g_df.pickle','wb')\n",
    "pickle.dump(original_g_df,f)\n",
    "f.close()\n",
    "\n",
    "f = open('data.pickle','wb')\n",
    "pickle.dump(data,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this cell to reload if needed\n",
    "#import pickle\n",
    "#f = open('original_g_df.pickle','rb')\n",
    "#original_g_df = pickle.load(f)\n",
    "#f.close()\n",
    "\n",
    "#f = open('data.pickle','rb')\n",
    "#data = pickle.load(f)\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_g_df_temp = original_g_df.copy()\n",
    "\n",
    "# empty list to store removable links\n",
    "omissible_links_index = []\n",
    "\n",
    "ctr = 0\n",
    "ctr2 = 0\n",
    "for i in original_g_df.index.values:\n",
    "    # remove a node pair and build a new graph\n",
    "    G_temp = nx.from_pandas_edgelist(original_g_df_temp.drop(i), \"node_1\", \"node_2\", create_using=nx.Graph())\n",
    "    \n",
    "    #print('len(G_temp.nodes): {}'.format(nx.number_of_nodes(G_temp)))\n",
    "    \n",
    "    ctr2 += 1\n",
    "    if ctr2 % 1000 == 0:\n",
    "        print(str(ctr2))\n",
    "    \n",
    "    # check that the number of nodes is same as the resulting graph\n",
    "    if (nx.number_of_nodes(G_temp) == initial_node_count):                                                            \n",
    "        omissible_links_index.append(i)\n",
    "        original_g_df_temp = original_g_df_temp.drop(index = i)\n",
    "        ctr += 1\n",
    "        if ctr % 500 == 0:\n",
    "            print('[info] Count of admissable links: {}'.format(ctr))\n",
    "        if ctr == 2000:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(omissible_links_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('omissible_links_index.pickle','wb')\n",
    "pickle.dump(omissible_links_index,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe of removable edges\n",
    "original_g_df_ghost = original_g_df.loc[omissible_links_index]\n",
    "\n",
    "# add the target variable 'link'\n",
    "original_g_df_ghost['link'] = 1\n",
    "\n",
    "# Reduce the dataset containing non-connected nodes to 4K so that there is a 1:2 ratio of connected nodes to non-connected nodes\n",
    "# The reduced dataset is a random sample without replacement\n",
    "data_reduced = data.sample(4000)\n",
    "\n",
    "data_reduced = data_reduced.append(original_g_df_ghost[['node_1', 'node_2', 'link']], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.get_vector('http://purl.obolibrary.org/obo/CHEBI_2430')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [model.wv.get_vector(i.toPython()) + model.wv.get_vector(j.toPython()) for i,j in zip(data_reduced['node_1'], data_reduced['node_2'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(np.array(x), \n",
    "                                                data_reduced['link'], \n",
    "                                                test_size = 0.3, \n",
    "                                                random_state = 35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two baseline classifiers with which to compare the LR using embedding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(xtrain, ytrain)\n",
    "predictions = dummy_clf.predict_proba(xtest)\n",
    "roc_auc_score(ytest, predictions[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_clf = DummyClassifier(strategy=\"stratified\")\n",
    "dummy_clf.fit(xtrain, ytrain)\n",
    "predictions = dummy_clf.predict_proba(xtest)\n",
    "roc_auc_score(ytest, predictions[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = dummy_clf.predict(xtest)\n",
    "d_f1 = f1_score(ytest, yhat)\n",
    "print('F1: {}'.format(d_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR using the node2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(class_weight=\"balanced\")\n",
    "lr.fit(xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lr.predict_proba(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(ytest, predictions[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = lr.predict(xtest)\n",
    "lr_f1 = f1_score(ytest, yhat)\n",
    "print('F1: {}'.format(lr_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(ytest, predictions[:,1])\n",
    "\n",
    "plt.plot(lr_recall, lr_precision, marker='.', label='Logistic')\n",
    "\n",
    "# axis labels\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "# show the legend\n",
    "plt.legend()\n",
    "# show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E9** - questions about graph embedding and link prediction\n",
    "\n",
    "1. List 3 applications of network embeddings in biomedicine. For each embedding application, name an ontology that you believe would be useful in constructing the knowledge graph that would be used to create the embeddings.\n",
    "\n",
    "2. Node embedding methods (such as node2vec and DeepWalk) use node features to embed graph information into vector space. Explain node features with a few examples.\n",
    "\n",
    "3. List a few data sources used to create the kg-covid-19 knowledge graph and find examples of triples that are included in the graph. \n",
    "\n",
    "4. Current graph representation learning (GRL) methods are not able to embed temporal graphs. Find an application in biomedicine where temporal graphs would be useful for representing knowledge.\n",
    "\n",
    "5. In this vignette, did we do link prediction on a homogeneous or heterogenious graph? \n",
    "\n",
    "6. Given its performance, would you trust for predicting novel links that are currently not in the graph? Please explain your answer. \n",
    "\n",
    "7. We did not tune any hyperparameters for the embedding. What options would you be interested in trying out?\n",
    "\n",
    "8. Some researchers have experimented with embedding knowledge graph nodes in hyperbolic geometry. Do some searching about this topic and write a sentence or two on what you find to be the main motivation of researchers for trying this out. Include citations.\n",
    "\n",
    "9. Write any comments or questions that you have about the embedding excercise in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ALL DONE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bionf2071",
   "language": "python",
   "name": "bionf2071"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
